<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Linear state-space model &mdash; BayesPy v0.2.3 Documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.2.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="BayesPy v0.2.3 Documentation" href="../index.html" />
    <link rel="up" title="Examples" href="examples.html" />
    <link rel="next" title="Developer guide" href="../dev_guide/dev_guide.html" />
    <link rel="prev" title="Principal component analysis" href="pca.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../dev_guide/dev_guide.html" title="Developer guide"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="pca.html" title="Principal component analysis"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">BayesPy v0.2.3 Documentation</a> &raquo;</li>
          <li><a href="examples.html" accesskey="U">Examples</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="linear-state-space-model">
<h1>Linear state-space model<a class="headerlink" href="#linear-state-space-model" title="Permalink to this headline">¶</a></h1>
<div class="section" id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h2>
<p>In linear state-space models a sequence of <span class="math">\(M\)</span>-dimensional observations
<span class="math">\(\mathbf{Y}=(\mathbf{y}_1,\ldots,\mathbf{y}_N)\)</span> is assumed to be generated
from latent <span class="math">\(D\)</span>-dimensional states
<span class="math">\(\mathbf{X}=(\mathbf{x}_1,\ldots,\mathbf{x}_N)\)</span> which follow a first-order
Markov process:</p>
<div class="math">
\[\begin{split}\begin{aligned}
\mathbf{x}_{n} &amp;= \mathbf{A}\mathbf{x}_{n-1} + \text{noise} \,,
\\
\mathbf{y}_{n} &amp;= \mathbf{C}\mathbf{x}_{n} + \text{noise} \,,
\end{aligned}\end{split}\]</div>
<p>where the noise is Gaussian, <span class="math">\(\mathbf{A}\)</span> is the <span class="math">\(D\times D\)</span> state
dynamics matrix and <span class="math">\(\mathbf{C}\)</span> is the <span class="math">\(M\times D\)</span> loading
matrix. Usually, the latent space dimensionality <span class="math">\(D\)</span> is assumed to be much
smaller than the observation space dimensionality <span class="math">\(M\)</span> in order to model
the dependencies of high-dimensional observations efficiently.</p>
<p>In order to construct the model in BayesPy, first import relevant nodes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="kn">import</span> <span class="n">GaussianARD</span><span class="p">,</span> <span class="n">GaussianMarkovChain</span><span class="p">,</span> <span class="n">Gamma</span><span class="p">,</span> <span class="n">Dot</span>
</pre></div>
</div>
<p>The data vectors will be 30-dimensional:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="mi">30</span>
</pre></div>
</div>
<p>There will be 400 data vectors:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">400</span>
</pre></div>
</div>
<p>Let us use 10-dimensional latent space:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>The state dynamics matrix <span class="math">\(\mathbf{A}\)</span> has ARD prior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s">&#39;alpha&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">alpha</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s">&#39;A&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <span class="math">\(\mathbf{A}\)</span> is a <span class="math">\(D\times{}D\)</span>-dimensional matrix.
However, in BayesPy it is modelled as a collection (<tt class="docutils literal"><span class="pre">plates=(D,)</span></tt>) of
<span class="math">\(D\)</span>-dimensional vectors (<tt class="docutils literal"><span class="pre">shape=(D,)</span></tt>) because this is how the variables
factorize in the posterior approximation of the state dynamics matrix in
<a class="reference internal" href="../user_api/generated/generated/bayespy.nodes.GaussianMarkovChain.html#bayespy.nodes.GaussianMarkovChain" title="bayespy.nodes.GaussianMarkovChain"><tt class="xref py py-class docutils literal"><span class="pre">GaussianMarkovChain</span></tt></a>.  The latent states are constructed as</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianMarkovChain</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="mf">1e-3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">A</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">name</span><span class="o">=</span><span class="s">&#39;X&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>where the first two arguments are the mean and precision matrix of the initial
state, the third argument is the state dynamics matrix and the fourth argument
is the diagonal elements of the precision matrix of the innovation noise.  The
node also needs the length of the chain given as the keyword argument <tt class="docutils literal"><span class="pre">n=N</span></tt>.
Thus, the shape of this node is <tt class="docutils literal"><span class="pre">(N,D)</span></tt>.</p>
<p>The linear mapping from the latent space to the observation space is modelled
with the loading matrix which has ARD prior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s">&#39;gamma&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">gamma</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s">&#39;C&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the plates for <tt class="docutils literal"><span class="pre">C</span></tt> are <tt class="docutils literal"><span class="pre">(M,1)</span></tt>, thus the full shape of the node is
<tt class="docutils literal"><span class="pre">(M,1,D)</span></tt>.  The unit plate axis is added so that <tt class="docutils literal"><span class="pre">C</span></tt> broadcasts with <tt class="docutils literal"><span class="pre">X</span></tt>
when computing the dot product:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">X</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">name</span><span class="o">=</span><span class="s">&#39;F&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This dot product is computed over the <span class="math">\(D\)</span>-dimensional latent space, thus
the result is a <span class="math">\(M\times{}N\)</span>-dimensional matrix which is now represented
with plates <tt class="docutils literal"><span class="pre">(M,N)</span></tt> in BayesPy:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(30, 400)</span>
</pre></div>
</div>
<p>We also need to use random initialization either for <tt class="docutils literal"><span class="pre">C</span></tt> or <tt class="docutils literal"><span class="pre">X</span></tt> in order to
find non-zero latent space because by default both <tt class="docutils literal"><span class="pre">C</span></tt> and <tt class="docutils literal"><span class="pre">X</span></tt> are
initialized to zero because of their prior distributions.  We use random
initialization for <tt class="docutils literal"><span class="pre">C</span></tt> and then we must update <tt class="docutils literal"><span class="pre">X</span></tt> the first time before
updating <tt class="docutils literal"><span class="pre">C</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>The precision of the observation noise is given gamma prior:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>            <span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">name</span><span class="o">=</span><span class="s">&#39;tau&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The observations are noisy versions of the dot products:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">F</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">tau</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s">&#39;Y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The variational Bayesian inference engine is then construced as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="kn">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <tt class="docutils literal"><span class="pre">X</span></tt> is given before <tt class="docutils literal"><span class="pre">C</span></tt>, thus <tt class="docutils literal"><span class="pre">X</span></tt> is updated before <tt class="docutils literal"><span class="pre">C</span></tt> by
default.</p>
</div>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>Now, let us generate some toy data for our model.  Our true latent space is four
dimensional with two noisy oscillator components, one random walk component and
one white noise component.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="p">),</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">0</span><span class="p">,</span>         <span class="mi">0</span><span class="p">,</span>          <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">0</span><span class="p">,</span>         <span class="mi">0</span><span class="p">,</span>          <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<p>The true linear mapping is just random:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, generate the latent states and the observations using the model equations:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">M</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">M</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">f</span><span class="p">[:,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>    <span class="n">y</span><span class="p">[:,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">[:,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
<p>We want to simulate missing values, thus we create a mask which randomly removes
80% of the data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.utils</span> <span class="kn">import</span> <span class="n">random</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>As we did not define plotters for our nodes when creating the model, it is done
now for some of the nodes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="kn">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">FunctionPlotter</span><span class="p">(</span><span class="n">center</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">HintonPlotter</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">HintonPlotter</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">PDFPlotter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)))</span>
</pre></div>
</div>
<p>This enables plotting of the approximate posterior distributions during VB
learning.  The inference engine can be run using <tt class="xref py py-func docutils literal"><span class="pre">VB.update()</span></tt> method:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-1.439704e+05 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 10: loglike=-1.051441e+04 (... seconds)</span>
</pre></div>
</div>
<p>The iteration progresses a bit slowly, thus we&#8217;ll consider parameter expansion
to speed it up.</p>
<div class="section" id="parameter-expansion">
<h3>Parameter expansion<a class="headerlink" href="#parameter-expansion" title="Permalink to this headline">¶</a></h3>
<p>Section <a class="reference internal" href="../user_guide/inference.html#sec-parameter-expansion"><em>Parameter expansion</em></a> discusses parameter expansion for
state-space models to speed up inference.  It is based on a rotating the latent
space such that the posterior in the observation space is not affected:</p>
<div class="math">
\[\mathbf{y}_n = \mathbf{C}\mathbf{x}_n =
(\mathbf{C}\mathbf{R}^{-1}) (\mathbf{R}\mathbf{x}_n) \,.\]</div>
<p>Thus, the transformation is
<span class="math">\(\mathbf{C}\rightarrow\mathbf{C}\mathbf{R}^{-1}\)</span> and
<span class="math">\(\mathbf{X}\rightarrow\mathbf{R}\mathbf{X}\)</span>.  In order to keep the
dynamics of the latent states unaffected by the transformation, the state
dynamics matrix <span class="math">\(\mathbf{A}\)</span> must be transformed accordingly:</p>
<div class="math">
\[\mathbf{R}\mathbf{x}_n = \mathbf{R}\mathbf{A}\mathbf{R}^{-1}
\mathbf{R}\mathbf{x}_{n-1} \,,\]</div>
<p>resulting in a transformation
<span class="math">\(\mathbf{A}\rightarrow\mathbf{R}\mathbf{A}\mathbf{R}^{-1}\)</span>.  For more
details, refer to <a class="reference internal" href="../references.html#luttinen-2013" id="id1">[6]</a> and <a class="reference internal" href="../references.html#luttinen-2010" id="id2">[7]</a>.  In BayesPy,
the transformations are available in
<tt class="xref py py-mod docutils literal"><span class="pre">bayespy.inference.vmp.transformations</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference.vmp</span> <span class="kn">import</span> <span class="n">transformations</span>
</pre></div>
</div>
<p>The rotation of the loading matrix along with the ARD parameters is defined as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rotC</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
<p>For rotating <tt class="docutils literal"><span class="pre">X</span></tt>, we first need to define the rotation of the state dynamics
matrix:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rotA</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can define the rotation of the latent states:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rotX</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianMarkovChain</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rotA</span><span class="p">)</span>
</pre></div>
</div>
<p>The optimal rotation for all these variables is found using rotation optimizer:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">R</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotationOptimizer</span><span class="p">(</span><span class="n">rotX</span><span class="p">,</span> <span class="n">rotC</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>Set the parameter expansion to be applied after each iteration:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">rotate</span>
</pre></div>
</div>
<p>Now, run iterations until convergence:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 11: loglike=-1.010806e+04 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 60: loglike=-8.906259e+03 (... seconds)</span>
<span class="go">Converged at iteration 60.</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p>Because we have set the plotters, we can plot those nodes as:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/lssm-1.py">Source code</a>)</p>
<div class="figure">
<img alt="../_images/lssm-1_00.png" src="../_images/lssm-1_00.png" />
<p class="caption">(<a class="reference external" href="../examples/lssm-1_00.png">png</a>, <a class="reference external" href="../examples/lssm-1_00.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_00.pdf">pdf</a>)</p>
</div>
<div class="figure">
<img alt="../_images/lssm-1_01.png" src="../_images/lssm-1_01.png" />
<p class="caption">(<a class="reference external" href="../examples/lssm-1_01.png">png</a>, <a class="reference external" href="../examples/lssm-1_01.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_01.pdf">pdf</a>)</p>
</div>
<div class="figure">
<img alt="../_images/lssm-1_02.png" src="../_images/lssm-1_02.png" />
<p class="caption">(<a class="reference external" href="../examples/lssm-1_02.png">png</a>, <a class="reference external" href="../examples/lssm-1_02.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_02.pdf">pdf</a>)</p>
</div>
<div class="figure">
<img alt="../_images/lssm-1_03.png" src="../_images/lssm-1_03.png" />
<p class="caption">(<a class="reference external" href="../examples/lssm-1_03.png">png</a>, <a class="reference external" href="../examples/lssm-1_03.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_03.pdf">pdf</a>)</p>
</div>
<p>There are clearly four effective components in <tt class="docutils literal"><span class="pre">X</span></tt>: random walk (component
number 1), random oscillation (7 and 10), and white noise (9).  These dynamics
are also visible in the state dynamics matrix Hinton diagram.  Note that the
white noise component does not have any dynamics.  Also <tt class="docutils literal"><span class="pre">C</span></tt> shows only four
effective components.  The posterior of <tt class="docutils literal"><span class="pre">tau</span></tt> captures the true value
<span class="math">\(3^{-2}\approx0.111\)</span> accurately.  We can also plot predictions in the
observation space:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/lssm-2.py">Source code</a>, <a class="reference external" href="../examples/lssm-2.png">png</a>, <a class="reference external" href="../examples/lssm-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="../_images/lssm-2.png" src="../_images/lssm-2.png" />
</div>
<p>We can also measure the performance numerically by computing root-mean-square
error (RMSE) of the missing values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.utils</span> <span class="kn">import</span> <span class="n">misc</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">misc</span><span class="o">.</span><span class="n">rmse</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">F</span><span class="o">.</span><span class="n">get_moments</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
<span class="go">5.182...</span>
</pre></div>
</div>
<p>This is relatively close to the standard deviation of the noise (3), so the
predictions are quite good considering that only 20% of the data was used.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Linear state-space model</a><ul>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#data">Data</a></li>
<li><a class="reference internal" href="#inference">Inference</a><ul>
<li><a class="reference internal" href="#parameter-expansion">Parameter expansion</a></li>
</ul>
</li>
<li><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pca.html"
                        title="previous chapter">Principal component analysis</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../dev_guide/dev_guide.html"
                        title="next chapter">Developer guide</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/examples/lssm.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../dev_guide/dev_guide.html" title="Developer guide"
             >next</a> |</li>
        <li class="right" >
          <a href="pca.html" title="Principal component analysis"
             >previous</a> |</li>
        <li><a href="../index.html">BayesPy v0.2.3 Documentation</a> &raquo;</li>
          <li><a href="examples.html" >Examples</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011-2014, Jaakko Luttinen, GPLv3.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>